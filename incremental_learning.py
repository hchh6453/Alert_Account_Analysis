"""
å¢é‡å­¸ç¿’ç•°å¸¸æª¢æ¸¬ç³»çµ±
1. ä½¿ç”¨å·²çŸ¥æ¨™ç±¤è³‡æ–™è¨“ç·´å¤šå€‹æ¨¡å‹
2. é€éäº¤å‰é©—è­‰æ¯”è¼ƒæ¨¡å‹æº–ç¢ºç‡
3. é¸æ“‡æœ€ä½³æ¨¡å‹é æ¸¬æœªçŸ¥æ¨™ç±¤è³‡æ–™
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import pickle
import os
from datetime import datetime

# åŒ¯å…¥å¿…è¦çš„æ¨¡çµ„
try:
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score
    from sklearn.ensemble import IsolationForest, RandomForestClassifier
    from sklearn.svm import OneClassSVM
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn.cluster import DBSCAN
    from sklearn.preprocessing import StandardScaler
    from sklearn.utils.class_weight import compute_class_weight
except ImportError as e:
    print(f"åŒ¯å…¥éŒ¯èª¤: {e}")
    print("è«‹åŸ·è¡Œ: pip install scikit-learn")
    exit(1)

from data_loader import DataLoader
from alert_focused_feature_engineering import AlertFocusedFeatureEngineer

class IncrementalDetector:
    """å¢é‡å­¸ç¿’ç•°å¸¸æª¢æ¸¬å™¨"""
    
    def __init__(self):
        self.feature_engineer = AlertFocusedFeatureEngineer()
        self.scaler = StandardScaler()
        self.models = {}
        self.best_model_name = None
        self.best_f1_score = 0.0
        
        print(f"å¢é‡å­¸ç¿’æª¢æ¸¬å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_training_data(self, transactions_df, alerts_df, predict_df, 
                           alert_ratio=0.3, sample_size=5000, strategy='balanced'):
        """å‰µå»ºè¨“ç·´è³‡æ–™é›†"""
        print(f"\n=== å‰µå»ºè¨“ç·´è³‡æ–™é›† ===")
        
        # å–å¾—å·²çŸ¥è­¦ç¤ºå¸³æˆ¶å’Œæ­£å¸¸å¸³æˆ¶
        alert_accounts = set(alerts_df['acct'])
        
        # å¾äº¤æ˜“è³‡æ–™ä¸­å–å¾—æ‰€æœ‰å¸³æˆ¶ï¼ˆfrom_acct å’Œ to_acctï¼‰
        all_from_accounts = set(transactions_df['from_acct'].unique())
        all_to_accounts = set(transactions_df['to_acct'].unique())
        all_accounts_in_txns = all_from_accounts.union(all_to_accounts)
        
        predict_accounts = set(predict_df['acct'])
        normal_accounts = all_accounts_in_txns - predict_accounts - alert_accounts
        
        print(f"å·²çŸ¥è­¦ç¤ºå¸³æˆ¶: {len(alert_accounts)}")
        print(f"å¯ç”¨æ­£å¸¸å¸³æˆ¶: {len(normal_accounts)}")
        
        # è¨ˆç®—éœ€è¦çš„å¸³æˆ¶æ•¸é‡
        target_alert_count = int(sample_size * alert_ratio)
        target_normal_count = sample_size - target_alert_count
        
        # æ¡æ¨£è­¦ç¤ºå¸³æˆ¶
        if len(alert_accounts) >= target_alert_count:
            sampled_alerts = list(alert_accounts)[:target_alert_count]
        else:
            sampled_alerts = list(alert_accounts) * (target_alert_count // len(alert_accounts) + 1)
            sampled_alerts = sampled_alerts[:target_alert_count]
        
        # æ¡æ¨£æ­£å¸¸å¸³æˆ¶ï¼ˆæ ¹æ“šç­–ç•¥ï¼‰
        if strategy == 'balanced':
            # ç­–ç•¥1ï¼šå¹³è¡¡æ¡æ¨£ï¼ˆç°¡å–®éš¨æ©Ÿï¼‰- å¿«é€Ÿä½†å¯èƒ½åå‘
            if len(normal_accounts) >= target_normal_count:
                sampled_normal = list(normal_accounts)[:target_normal_count]
            else:
                sampled_normal = list(normal_accounts)
                
        elif strategy == 'diverse':
            # ç­–ç•¥2ï¼šå¤šæ¨£åŒ–æ¡æ¨£ï¼ˆç¢ºä¿ä¸åŒé¡å‹çš„æ­£å¸¸å¸³æˆ¶ï¼‰- éš¨æ©Ÿæ€§å¼·
            normal_list = list(normal_accounts)
            if len(normal_list) >= target_normal_count:
                # éš¨æ©Ÿæ¡æ¨£ç¢ºä¿å¤šæ¨£æ€§
                import random
                random.seed(42)
                sampled_normal = random.sample(normal_list, target_normal_count)
            else:
                sampled_normal = normal_list
                
        elif strategy == 'active':
            # ç­–ç•¥3ï¼šæ´»èºå¸³æˆ¶å„ªå…ˆï¼ˆäº¤æ˜“æ¬¡æ•¸å¤šçš„å¸³æˆ¶ï¼‰- é‡è¦–äº¤æ˜“é »ç¹çš„å¸³æˆ¶
            # è¨ˆç®—æ¯å€‹æ­£å¸¸å¸³æˆ¶çš„äº¤æ˜“æ¬¡æ•¸
            normal_txn_counts = {}
            for account in list(normal_accounts)[:10000]:  # é™åˆ¶è¨ˆç®—ç¯„åœé¿å…å¤ªæ…¢
                account_txns = self.feature_engineer.get_account_transactions(account, transactions_df)
                normal_txn_counts[account] = len(account_txns)
            
            # æŒ‰äº¤æ˜“æ¬¡æ•¸æ’åºï¼Œé¸æ“‡æœ€æ´»èºçš„å¸³æˆ¶
            sorted_accounts = sorted(normal_txn_counts.items(), key=lambda x: x[1], reverse=True)
            sampled_normal = [account for account, count in sorted_accounts[:target_normal_count]]
            
        elif strategy == 'representative':
            # ç­–ç•¥4ï¼šä»£è¡¨æ€§æ¡æ¨£ï¼ˆç¢ºä¿æ¶µè“‹ä¸åŒäº¤æ˜“æ¨¡å¼ï¼‰
            # å…ˆéš¨æ©Ÿé¸æ“‡ä¸€éƒ¨åˆ†å¸³æˆ¶ï¼Œåˆ†æå…¶äº¤æ˜“ç‰¹å¾µ
            import random
            random.seed(42)
            sample_size_for_analysis = min(5000, len(normal_accounts))
            analysis_accounts = random.sample(list(normal_accounts), sample_size_for_analysis)
            
            # è¨ˆç®—é€™äº›å¸³æˆ¶çš„äº¤æ˜“ç‰¹å¾µ
            account_features = []
            for account in analysis_accounts:
                account_txns = self.feature_engineer.get_account_transactions(account, transactions_df)
                if len(account_txns) > 0:
                    avg_amount = account_txns['txn_amt'].mean()
                    txn_count = len(account_txns)
                    account_features.append((account, avg_amount, txn_count))
            
            # æŒ‰ç‰¹å¾µåˆ†çµ„ï¼Œå¾æ¯çµ„ä¸­é¸æ“‡ä»£è¡¨æ€§å¸³æˆ¶
            account_features.sort(key=lambda x: x[1])  # æŒ‰å¹³å‡é‡‘é¡æ’åº
            group_size = len(account_features) // 10  # åˆ†æˆ10çµ„
            sampled_normal = []
            
            for i in range(0, len(account_features), group_size):
                group = account_features[i:i+group_size]
                if group:
                    # å¾æ¯çµ„ä¸­éš¨æ©Ÿé¸æ“‡
                    selected = random.choice(group)
                    sampled_normal.append(selected[0])
                    if len(sampled_normal) >= target_normal_count:
                        break
            
            # å¦‚æœé‚„ä¸å¤ ï¼Œéš¨æ©Ÿè£œå……
            if len(sampled_normal) < target_normal_count:
                remaining_needed = target_normal_count - len(sampled_normal)
                remaining_accounts = list(normal_accounts - set(sampled_normal))
                additional = random.sample(remaining_accounts, min(remaining_needed, len(remaining_accounts)))
                sampled_normal.extend(additional)
        else:
            # é è¨­ç­–ç•¥
            if len(normal_accounts) >= target_normal_count:
                sampled_normal = list(normal_accounts)[:target_normal_count]
            else:
                sampled_normal = list(normal_accounts)
        
        print(f"æ¡æ¨£è­¦ç¤ºå¸³æˆ¶: {len(sampled_alerts)}")
        print(f"æ¡æ¨£æ­£å¸¸å¸³æˆ¶: {len(sampled_normal)}")
        
        # å»ºç«‹ç‰¹å¾µ
        training_features = []
        
        print("å»ºç«‹è­¦ç¤ºå¸³æˆ¶ç‰¹å¾µ...")
        for i, account in enumerate(sampled_alerts):
            if i % 100 == 0:
                print(f"  é€²åº¦: {i+1}/{len(sampled_alerts)}")
            
            features = self.feature_engineer.create_alert_focused_features(
                account, transactions_df, alerts_df, predict_df
            )
            training_features.append(features)
        
        print("å»ºç«‹æ­£å¸¸å¸³æˆ¶ç‰¹å¾µ...")
        for i, account in enumerate(sampled_normal):
            if i % 100 == 0:
                print(f"  é€²åº¦: {i+1}/{len(sampled_normal)}")
            
            features = self.feature_engineer.create_alert_focused_features(
                account, transactions_df, alerts_df, predict_df
            )
            training_features.append(features)
        
        # è½‰æ›ç‚º DataFrame
        training_df = pd.DataFrame(training_features)
        feature_cols = [col for col in training_df.columns if col not in ['acct', 'label']]
        X_train = training_df[feature_cols]
        y_train = training_df['label']
        
        print(f"è¨“ç·´è³‡æ–™å½¢ç‹€: {X_train.shape}")
        print(f"æ¨™ç±¤åˆ†å¸ƒ: {y_train.value_counts().to_dict()}")
        
        return X_train, y_train, feature_cols
    
    def train_and_compare_models(self, X_train, y_train):
        """è¨“ç·´ä¸¦æ¯”è¼ƒå¤šå€‹æ¨¡å‹"""
        print(f"\n=== è¨“ç·´ä¸¦æ¯”è¼ƒæ¨¡å‹ ===")
        
        # è™•ç†è³‡æ–™
        X_train_clean = X_train.fillna(0)
        X_train_scaled = self.scaler.fit_transform(X_train_clean)
        
        # åˆ†å‰²è¨“ç·´å’Œé©—è­‰è³‡æ–™
        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
            X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        print(f"è¨“ç·´è³‡æ–™: {len(X_train_split)} ç­†")
        print(f"é©—è­‰è³‡æ–™: {len(X_val_split)} ç­†")
        
        # ä½¿ç”¨äº¤å‰é©—è­‰è©•ä¼°æ¨¡å‹
        print(f"\n=== ä½¿ç”¨äº¤å‰é©—è­‰è©•ä¼°æ¨¡å‹ ===")
        cv_scores = {}
        
        # 1. Random Forest
        print("1. è©•ä¼° Random Forest...")
        rf = RandomForestClassifier(
            n_estimators=100,
            max_depth=5,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            class_weight='balanced'  # é‡æ–°å•Ÿç”¨é¡åˆ¥æ¬Šé‡
        )
        cv_f1_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5, scoring='f1')
        cv_scores['random_forest'] = cv_f1_scores.mean()
        print(f"  äº¤å‰é©—è­‰ F1-Score: {cv_f1_scores.mean():.4f} (+/- {cv_f1_scores.std() * 2:.4f})")
        
        rf.fit(X_train_split, y_train_split)
        self.models['random_forest'] = rf
        
        # 2. Isolation Forest
        print("2. è©•ä¼° Isolation Forest...")
        iso_forest = IsolationForest(
            contamination=0.1,  # 10% ç•°å¸¸æ¯”ä¾‹ï¼ˆå¹³è¡¡å­¸ç¿’ï¼‰
            random_state=42,
            n_estimators=100
        )
        iso_predictions = iso_forest.fit_predict(X_train_scaled)
        iso_predictions = np.where(iso_predictions == -1, 1, 0)
        iso_f1 = f1_score(y_train, iso_predictions, average='binary')
        cv_scores['isolation_forest'] = iso_f1
        print(f"  è¨“ç·´è³‡æ–™ F1-Score: {iso_f1:.4f}")
        
        iso_forest.fit(X_train_split)
        self.models['isolation_forest'] = iso_forest
        
        # 3. One-Class SVM
        print("3. è©•ä¼° One-Class SVM...")
        oc_svm = OneClassSVM(
            nu=0.1,  # 10% ç•°å¸¸æ¯”ä¾‹ï¼ˆå¹³è¡¡å­¸ç¿’ï¼‰
            kernel='rbf',
            gamma='scale'
        )
        oc_svm.fit(X_train_scaled)
        oc_predictions = oc_svm.predict(X_train_scaled)
        oc_predictions = np.where(oc_predictions == -1, 1, 0)
        oc_f1 = f1_score(y_train, oc_predictions, average='binary')
        cv_scores['one_class_svm'] = oc_f1
        print(f"  è¨“ç·´è³‡æ–™ F1-Score: {oc_f1:.4f}")
        
        self.models['one_class_svm'] = oc_svm
        
        # 4. Local Outlier Factor
        print("4. è©•ä¼° Local Outlier Factor...")
        lof = LocalOutlierFactor(
            n_neighbors=20,
            contamination=0.1,  # 10% ç•°å¸¸æ¯”ä¾‹ï¼ˆå¹³è¡¡å­¸ç¿’ï¼‰
            novelty=True
        )
        lof.fit(X_train_scaled)
        lof_predictions = lof.predict(X_train_scaled)
        lof_predictions = np.where(lof_predictions == -1, 1, 0)
        lof_f1 = f1_score(y_train, lof_predictions, average='binary')
        cv_scores['lof'] = lof_f1
        print(f"  è¨“ç·´è³‡æ–™ F1-Score: {lof_f1:.4f}")
        
        self.models['lof'] = lof
        
        # 5. DBSCAN
        print("5. è©•ä¼° DBSCAN...")
        dbscan = DBSCAN(
            eps=0.5,
            min_samples=5
        )
        dbscan.fit(X_train_scaled)
        dbscan_predictions = dbscan.fit_predict(X_train_scaled)
        dbscan_predictions = np.where(dbscan_predictions == -1, 1, 0)
        dbscan_f1 = f1_score(y_train, dbscan_predictions, average='binary')
        cv_scores['dbscan'] = dbscan_f1
        print(f"  è¨“ç·´è³‡æ–™ F1-Score: {dbscan_f1:.4f}")
        
        self.models['dbscan'] = dbscan
        
        # é¸æ“‡æœ€ä½³æ¨¡å‹
        best_model_name = max(cv_scores.keys(), key=lambda x: cv_scores[x])
        best_f1_score = cv_scores[best_model_name]
        
        self.best_model_name = best_model_name
        self.best_f1_score = best_f1_score
        
        print(f"\nğŸ¯ æœ€ä½³æ¨¡å‹: {best_model_name}")
        print(f"ğŸ¯ æœ€ä½³ F1-Score: {best_f1_score:.4f}")
        
        # é¡¯ç¤ºæ‰€æœ‰æ¨¡å‹æ’å
        print(f"\nğŸ“Š æ¨¡å‹æ’å (æŒ‰ F1-Score):")
        sorted_models = sorted(cv_scores.items(), key=lambda x: x[1], reverse=True)
        for i, (model_name, score) in enumerate(sorted_models, 1):
            print(f"  {i}. {model_name}: {score:.4f}")
        
        # åœ¨é©—è­‰è³‡æ–™ä¸Šè©•ä¼°æœ€ä½³æ¨¡å‹
        print(f"\n=== é©—è­‰æœ€ä½³æ¨¡å‹ {best_model_name} ===")
        self.evaluate_best_model(X_val_split, y_val_split)
    
    def evaluate_best_model(self, X_val, y_val):
        """è©•ä¼°æœ€ä½³æ¨¡å‹"""
        best_model = self.models[self.best_model_name]
        
        try:
            if self.best_model_name == 'random_forest':
                predictions = best_model.predict(X_val)
                scores = best_model.predict_proba(X_val)[:, 1]
            elif self.best_model_name == 'isolation_forest':
                predictions = best_model.predict(X_val)
                predictions = np.where(predictions == -1, 1, 0)
                scores = best_model.decision_function(X_val)
            elif self.best_model_name == 'one_class_svm':
                predictions = best_model.predict(X_val)
                predictions = np.where(predictions == -1, 1, 0)
                scores = best_model.decision_function(X_val)
            elif self.best_model_name == 'lof':
                predictions = best_model.predict(X_val)
                predictions = np.where(predictions == -1, 1, 0)
                scores = best_model.decision_function(X_val)
            elif self.best_model_name == 'dbscan':
                predictions = best_model.fit_predict(X_val)
                predictions = np.where(predictions == -1, 1, 0)
                scores = -np.abs(predictions)
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            if len(np.unique(y_val)) > 1:
                f1 = f1_score(y_val, predictions, average='binary')
                precision = precision_score(y_val, predictions, average='binary')
                recall = recall_score(y_val, predictions, average='binary')
            else:
                f1 = precision = recall = 0.0
            
            print(f"é©—è­‰çµæœ:")
            print(f"  F1-Score: {f1:.4f}")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall: {recall:.4f}")
            
            # é¡¯ç¤ºæ··æ·†çŸ©é™£
            cm = confusion_matrix(y_val, predictions)
            print(f"  æ··æ·†çŸ©é™£: {cm}")
            
        except Exception as e:
            print(f"âŒ è©•ä¼°æœ€ä½³æ¨¡å‹å¤±æ•—: {e}")
    
    def predict_with_best_model(self, X_test, feature_cols):
        """ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œé æ¸¬"""
        if self.best_model_name is None:
            print("âŒ æ²’æœ‰å¯ç”¨çš„æœ€ä½³æ¨¡å‹")
            return None
        
        print(f"ğŸ¯ ä½¿ç”¨æœ€ä½³æ¨¡å‹ {self.best_model_name} é€²è¡Œé æ¸¬...")
        
        X_test_clean = X_test.fillna(0)
        X_test_scaled = self.scaler.transform(X_test_clean)
        
        best_model = self.models[self.best_model_name]
        
        if self.best_model_name == 'random_forest':
            predictions = best_model.predict(X_test_scaled)
            scores = best_model.predict_proba(X_test_scaled)[:, 1]
        elif self.best_model_name == 'isolation_forest':
            predictions = best_model.predict(X_test_scaled)
            predictions = np.where(predictions == -1, 1, 0)
            scores = best_model.decision_function(X_test_scaled)
        elif self.best_model_name == 'one_class_svm':
            predictions = best_model.predict(X_test_scaled)
            predictions = np.where(predictions == -1, 1, 0)
            scores = best_model.decision_function(X_test_scaled)
        elif self.best_model_name == 'lof':
            predictions = best_model.predict(X_test_scaled)
            predictions = np.where(predictions == -1, 1, 0)
            scores = best_model.decision_function(X_test_scaled)
        elif self.best_model_name == 'dbscan':
            predictions = best_model.fit_predict(X_test_scaled)
            predictions = np.where(predictions == -1, 1, 0)
            scores = -np.abs(predictions)
        
        return predictions
    
    def predict_all_accounts(self, transactions_df, alerts_df, predict_df, feature_cols):
        """é æ¸¬æ‰€æœ‰éœ€è¦é æ¸¬çš„å¸³æˆ¶"""
        print(f"\n=== é æ¸¬æ‰€æœ‰å¸³æˆ¶ ===")
        
        predict_accounts = list(predict_df['acct'])
        batch_size = 1000
        
        all_predictions = []
        all_accounts = []
        
        for batch_num in range(0, len(predict_accounts), batch_size):
            batch_accounts = predict_accounts[batch_num:batch_num + batch_size]
            print(f"\nè™•ç†æ‰¹æ¬¡ {batch_num // batch_size + 1}: {len(batch_accounts)} å€‹å¸³æˆ¶")
            
            # å»ºç«‹ç‰¹å¾µ
            batch_features = []
            for i, account in enumerate(batch_accounts):
                if i % 100 == 0:
                    print(f"  é€²åº¦: {i+1}/{len(batch_accounts)}")
                
                features = self.feature_engineer.create_alert_focused_features(
                    account, transactions_df, alerts_df, predict_df
                )
                batch_features.append(features)
            
            batch_df = pd.DataFrame(batch_features)
            X_batch = batch_df[feature_cols]
            
            # ä½¿ç”¨æœ€ä½³æ¨¡å‹é æ¸¬
            predictions = self.predict_with_best_model(X_batch, feature_cols)
            
            if predictions is not None:
                all_predictions.extend(predictions)
                all_accounts.extend(batch_accounts)
                
                print(f"é æ¸¬çµæœ: {predictions.sum()} å€‹è­¦ç¤ºå¸³æˆ¶")
        
        return all_predictions, all_accounts

def run_incremental_learning():
    """é‹è¡Œå¢é‡å­¸ç¿’"""
    print("=== å¢é‡å­¸ç¿’ç•°å¸¸æª¢æ¸¬ç³»çµ± ===")
    
    # 1. è¼‰å…¥è³‡æ–™
    print("\n1. è¼‰å…¥è³‡æ–™...")
    data_loader = DataLoader()
    transactions_df, alerts_df, predict_df = data_loader.load_data()
    
    print(f"åŸå§‹äº¤æ˜“è³‡æ–™: {len(transactions_df):,} ç­†")
    print(f"è­¦ç¤ºå¸³æˆ¶: {len(alerts_df):,} å€‹")
    print(f"é æ¸¬ç›®æ¨™: {len(predict_df):,} å€‹")
    
    # 2. åˆå§‹åŒ–æª¢æ¸¬å™¨
    print("\n2. åˆå§‹åŒ–æª¢æ¸¬å™¨...")
    detector = IncrementalDetector()
    
    # 3. å‰µå»ºè¨“ç·´è³‡æ–™
    X_train, y_train, feature_cols = detector.create_training_data(
        transactions_df, alerts_df, predict_df,
        alert_ratio=0.01,  # 1% è­¦ç¤ºå¸³æˆ¶ï¼ˆå¹³è¡¡å­¸ç¿’å’ŒçœŸå¯¦æ€§ï¼‰
        sample_size=5000,  # 5000 å€‹è¨“ç·´æ¨£æœ¬ï¼ˆæ¸›å°‘è¨ˆç®—æ™‚é–“ï¼‰
        strategy='representative'  # è¿½æ±‚å…¨é¢æ€§ï¼šç¢ºä¿æ¶µè“‹ä¸åŒäº¤æ˜“æ¨¡å¼
        # å…¶ä»–ç­–ç•¥é¸é …ï¼ˆå·²è¨»è§£ä¿ç•™ï¼‰:
        # strategy='balanced'   # å¹³è¡¡æ¡æ¨£ï¼šç°¡å–®éš¨æ©Ÿé¸æ“‡
        # strategy='diverse'    # å¤šæ¨£åŒ–æ¡æ¨£ï¼šéš¨æ©Ÿæ¡æ¨£ç¢ºä¿å¤šæ¨£æ€§
        # strategy='active'     # æ´»èºå¸³æˆ¶å„ªå…ˆï¼šé¸æ“‡äº¤æ˜“æ¬¡æ•¸å¤šçš„å¸³æˆ¶
    )
    
    # 4. è¨“ç·´ä¸¦æ¯”è¼ƒæ¨¡å‹
    detector.train_and_compare_models(X_train, y_train)
    
    # 5. ä½¿ç”¨æœ€ä½³æ¨¡å‹é æ¸¬æ‰€æœ‰å¸³æˆ¶
    all_predictions, all_accounts = detector.predict_all_accounts(
        transactions_df, alerts_df, predict_df, feature_cols
    )
    
    # 6. è¼¸å‡ºçµæœ
    if all_predictions:
        print(f"\n=== æœ€çµ‚çµæœ ===")
        
        # å»ºç«‹ç¬¦åˆ submission_template æ ¼å¼çš„çµæœ
        submission_df = pd.DataFrame({
            'acct': all_accounts,
            'label': all_predictions
        })
        
        # å„²å­˜çµæœ
        submission_df.to_csv('submission.csv', index=False)
        
        print(f"é æ¸¬çµæœå·²å„²å­˜è‡³ submission.csv")
        print(f"ç¸½é æ¸¬è­¦ç¤ºå¸³æˆ¶: {sum(all_predictions):,}")
        print(f"ç¸½é æ¸¬æ­£å¸¸å¸³æˆ¶: {len(all_predictions) - sum(all_predictions):,}")
        
        # é¡¯ç¤ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
        print(f"\næœ€çµ‚æœ€ä½³æ¨¡å‹: {detector.best_model_name}")
        print(f"æœ€çµ‚æœ€ä½³ F1-Score: {detector.best_f1_score:.4f}")
        
        # é©—è­‰å®Œæ•´æ€§
        predict_accounts = set(predict_df['acct'])
        submission_accounts = set(submission_df['acct'])
        
        if predict_accounts == submission_accounts:
            print("âœ… é æ¸¬å®Œæ•´æ€§é©—è­‰é€šé")
        else:
            print("âŒ é æ¸¬å®Œæ•´æ€§é©—è­‰å¤±æ•—")
            print(f"ç¼ºå°‘: {len(predict_accounts - submission_accounts)}")
            print(f"å¤šé¤˜: {len(submission_accounts - predict_accounts)}")
    
    return detector

if __name__ == "__main__":
    run_incremental_learning()